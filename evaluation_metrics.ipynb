{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evaluation metrics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO3D40oYXmhomgX+Y4sMqS1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkhithaBabu/DL/blob/master/evaluation_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXb-lcfp1zlU"
      },
      "source": [
        "#**EVALUATION METRICS**\n",
        "Different types of evaluation metrics available.\n",
        "* Classification Accuracy\n",
        "* Logarithmic Loss\n",
        "* Confusion Matrix\n",
        "* Area under Curve\n",
        "* F1 Score\n",
        "* Mean Absolute Error\n",
        "* Mean Squared Error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjVWu58B92kF"
      },
      "source": [
        "##Classification Accuracy\n",
        "Classification Accuracy is what we usually mean, when we use the term accuracy. It is the ratio of number of correct predictions to the total number of input samples.\n",
        "\n",
        "*Note: It works well only if there are equal number of samples belonging to each class.The real problem arises, when the cost of misclassification of the minor class samples are very high.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_OIMTLsBrwC"
      },
      "source": [
        "##Logarithmic Loss\n",
        "Logarithmic Loss or Log Loss, works by penalising the false classifications. It works well for multi-class classification. When working with Log Loss, the classifier must assign probability to each class for all the samples. \n",
        "\n",
        "Log Loss has no upper bound and it exists on the range [0, ∞). Log Loss nearer to 0 indicates higher accuracy, whereas if the Log Loss is away from 0 then it indicates lower accuracy.\n",
        "In general, minimising Log Loss gives greater accuracy for the classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAiHVi9rBn1Z"
      },
      "source": [
        "##Confusion Matrix\n",
        "Confusion Matrix as the name suggests gives us a matrix as output and describes the complete performance of the model.\n",
        "\n",
        "###True Positives (TP) : \n",
        "The cases in which we predicted YES and the actual output was also YES.\n",
        "###True Negatives (TN) : \n",
        "The cases in which we predicted NO and the actual output was NO.\n",
        "###False Positives (FP) : \n",
        "The cases in which we predicted YES and the actual output was NO.\n",
        "###False Negatives (FN) : \n",
        "The cases in which we predicted NO and the actual output was YES."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqDG65hdBj0k"
      },
      "source": [
        "##Area Under Curve (AUC)\n",
        "Area Under Curve(AUC) is one of the most widely used metrics for evaluation. It is used for binary classification problem. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example. Before defining AUC, let us understand two basic terms :\n",
        "###True Positive [TP] Rate (Sensitivity) : \n",
        "True Positive Rate is defined as TP/ (FN+TP). True Positive Rate corresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points.\n",
        "###True Negative [TN] Rate (Specificity) : \n",
        "True Negative Rate is defined as TN / (FP+TN). False Positive Rate corresponds to the proportion of negative data points that are correctly considered as negative, with respect to all negative data points.\n",
        "###False Positive [FP] Rate : \n",
        "False Positive Rate is defined as FP / (FP+TN). False Positive Rate corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points.\n",
        "\n",
        "False Positive Rate and True Positive Rate both have values in the range [0, 1]. FPR and TPR both are computed at varying threshold values such as (0.00, 0.02, 0.04, …., 1.00) and a graph is drawn. \n",
        "\n",
        "AUC is the area under the curve of plot False Positive Rate vs True Positive Rate at different points in [0, 1]. As evident, AUC has a range of [0, 1]. The greater the value, the better is the performance of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHkdS1Y4-hpW"
      },
      "source": [
        "##F1 Score\n",
        "F1 Score is the Harmonic Mean between precision and recall. The range for F1 Score is [0, 1]. It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances).\n",
        "High precision but lower recall, gives you an extremely accurate, but it then misses a large number of instances that are difficult to classify. The greater the F1 Score, the better is the performance of our model.\n",
        "###Precision:\n",
        "It is the number of correct positive results divided by the number of positive results predicted by the classifier.\n",
        "###Recall:\n",
        "It is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5Xi2mdH92qh"
      },
      "source": [
        "##Mean Absolute Error\n",
        "Mean Absolute Error is the average of the difference between the Original Values and the Predicted Values. It gives us the measure of how far the predictions were from the actual output. However, they don’t gives us any idea of the direction of the error i.e. whether we are under predicting the data or over predicting the data. Mathematically, it is represented as :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGAOG0yy92wf"
      },
      "source": [
        "##Mean Squared Error\n",
        "Mean Squared Error(MSE) is quite similar to Mean Absolute Error, the only difference being that MSE takes the average of the square of the difference between the original values and the predicted values. The advantage of MSE being that it is easier to compute the gradient, whereas Mean Absolute Error requires complicated linear programming tools to compute the gradient. As, we take square of the error, the effect of larger errors become more pronounced then smaller error, hence the model can now focus more on the larger errors.\n",
        "Image for post"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUrKzj22E7Af"
      },
      "source": [
        "##Gain and Lift charts\n",
        "Gain and Lift chart are mainly concerned to check the rank ordering of the probabilities. Here are the steps to build a Lift/Gain chart:\n",
        "\n",
        "Step 1 : Calculate probability for each observation\n",
        "\n",
        "Step 2 : Rank these probabilities in decreasing order.\n",
        "\n",
        "Step 3 : Build deciles with each group having almost 10% of the observations.\n",
        "\n",
        "Step 4 : Calculate the response rate at each deciles for Good (Responders) ,Bad (Non-responders) and total.\n",
        "\n",
        "\n",
        "\n",
        "Lift / Gain charts are widely used in campaign targeting problems. This tells us till which decile can we target customers for an specific campaign. Also, it tells you how much response do you expect from the new target base.\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSigWf-vl0dJ"
      },
      "source": [
        "## Gini Coefficient\n",
        "Gini coefficient is sometimes used in classification problems. Gini coefficient can be straigh away derived from the AUC ROC number. Gini is nothing but ratio between area between the ROC curve and the diagnol line & the area of the above triangle. Following is the formulae used :\n",
        "\n",
        "Gini = 2*AUC – 1\n",
        "\n",
        "Gini above 60% is a good model. For the case in hand we get Gini as 92.7%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3iiJ6VRkzFK"
      },
      "source": [
        "## Concordant – Discordant ratio\n",
        "This is again one of the most important metric for any classification predictions problem. To understand this let’s assume we have 3 students who have some likelihood to pass this year. Following are our predictions :\n",
        "\n",
        "A – 0.9\n",
        "\n",
        "B – 0.5\n",
        "\n",
        "C – 0.3\n",
        "\n",
        "Now picture this. if we were to fetch pairs of two from these three student, how many pairs will we have? We will have 3 pairs : AB , BC, CA. Now, after the year ends we saw that A and C passed this year while B failed. No, we choose all the pairs where we will find one responder and other non-responder. How many such pairs do we have?\n",
        "\n",
        "We have two pairs AB and BC. Now for each of the 2 pairs, the concordant pair is where the probability of responder was higher than non-responder. Whereas discordant pair is where the vice-versa holds true. In case both the probabilities were equal, we say its a tie. Let’s see what happens in our case :\n",
        "\n",
        "AB  – Concordant\n",
        "\n",
        "BC – Discordant\n",
        "\n",
        "Hence, we have 50% of concordant cases in this example. Concordant ratio of more than 60% is considered to be a good model. This metric generally is not used when deciding how many customer to target etc. It is primarily used to access the model’s predictive power. For decisions like how many to target are again taken by KS / Lift charts.\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk2nhkyDlfs9"
      },
      "source": [
        "## Root Mean Squared Error (RMSE)\n",
        "RMSE is the most popular evaluation metric used in regression problems. It follows an assumption that error are unbiased and follow a normal distribution. Here are the key points to consider on RMSE:\n",
        "\n",
        "The power of ‘square root’  empowers this metric to show large number deviations.\n",
        "The ‘squared’ nature of this metric helps to deliver more robust results which prevents cancelling the positive and negative error values. In other words, this metric aptly displays the plausible magnitude of error term.\n",
        "It avoids the use of absolute error values which is highly undesirable in mathematical calculations.\n",
        "When we have more samples, reconstructing the error distribution using RMSE is considered to be more reliable.\n",
        "RMSE is highly affected by outlier values. Hence, make sure you’ve removed outliers from your data set prior to using this metric.\n",
        "As compared to mean absolute error, RMSE gives higher weightage and punishes large errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWuXeThtlb2N"
      },
      "source": [
        "##Root Mean Squared Logarithmic Error\n",
        "In case of Root mean squared logarithmic error, we take the log of the predictions and actual values. So basically, what changes are the variance that we are measuring. RMSLE is usually used when we don’t want to penalize huge differences in the predicted and the actual values when both predicted and true values are huge numbers.\n",
        "\n",
        "\n",
        "\n",
        "If both predicted and actual values are small: RMSE and RMSLE are same.\n",
        "If either predicted or the actual value is big: RMSE > RMSLE\n",
        "If both predicted and actual values are big: RMSE > RMSLE (RMSLE becomes almost negligible)\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXoZQxw9lWCA"
      },
      "source": [
        "##R-Squared/Adjusted R-Squared\n",
        "We learned that when the RMSE decreases, the model’s performance will improve. But these values alone are not intuitive.\n",
        "\n",
        "In the case of a classification problem, if the model has an accuracy of 0.8, we could gauge how good our model is against a random model, which has an accuracy of  0.5. So the random model can be treated as a benchmark. But when we talk about the RMSE metrics, we do not have a benchmark to compare.\n",
        "\n",
        "\n",
        "\n",
        "###Adjusted R-Squared\n",
        "A model performing equal to baseline would give R-Squared as 0. Better the model, higher the r2 value. The best model with all correct predictions would give R-Squared as 1. However, on adding new features to the model, the R-Squared value either increases or remains the same. R-Squared does not penalize for adding features that add no value to the model. So an improved version over the R-Squared is the adjusted R-Squared. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnFeRT_nkahX"
      },
      "source": [
        "## Kolomogorov Smirnov chart\n",
        "K-S or Kolmogorov-Smirnov chart measures performance of classification models. More accurately, K-S is a measure of the degree of separation between the positive and negative distributions. The K-S is 100, if the scores partition the population into two separate groups in which one group contains all the positives and the other all the negatives.\n",
        "\n",
        "On the other hand, If the model cannot differentiate between positives and negatives, then it is as if the model selects cases randomly from the population. The K-S would be 0. In most classification models the K-S will fall between 0 and 100, and that the higher the value the better the model is at separating the positive from negative cases."
      ]
    }
  ]
}